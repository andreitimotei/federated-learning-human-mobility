2025-04-14 18:55:18.971447: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-14 18:55:19.063448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744653319.078858  174046 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744653319.083083  174046 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744653319.142303  174046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744653319.142361  174046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744653319.142364  174046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744653319.142366  174046 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-14 18:55:19.148536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=50, no round_timeout
2025-04-14 18:55:25,273	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 1519956788.0, 'object_store_memory': 759978393.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 6, 'num_gpus': 1}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=174932)[0m 2025-04-14 18:55:27.104700: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=174932)[0m 2025-04-14 18:55:27.138341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=174932)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=174932)[0m E0000 00:00:1744653327.149808  174932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=174932)[0m E0000 00:00:1744653327.152974  174932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=174932)[0m W0000 00:00:1744653327.166548  174932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=174932)[0m W0000 00:00:1744653327.166570  174932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=174932)[0m W0000 00:00:1744653327.166572  174932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=174932)[0m W0000 00:00:1744653327.166573  174932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=174932)[0m 2025-04-14 18:55:27.169915: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=174932)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=174932)[0m I0000 00:00:1744653330.750094  174932 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 21]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 22]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 23]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 24]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 25]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 26]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 27]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 28]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 29]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 30]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 31]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 32]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 33]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 34]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 35]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 36]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 37]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 38]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 39]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 40]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 41]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 42]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 43]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 44]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 45]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 46]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 47]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 48]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 49]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 50]
[92mINFO [0m:      configure_fit: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 5 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 5 clients (out of 5)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: c6a8e74c744e461d95d108566dcf0e57997f9631b2cedea7363443b9) where the task (actor ID: 64431cc26bb27a023599ce2a01000000, name=ClientAppActor.__init__, pid=174932, memory used=0.70GB) was running was 22.28GB / 22.49GB (0.990642), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-acf1e72b7c580299ee1f2ca61336a7106e33e1656edcb7e1fc573ac6*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
28852	12.54	ray::ClientAppActor.run
165565	1.74	ray::ClientAppActor.run
742	0.85	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
174932	0.70	ray::ClientAppActor.run
164636	0.69	python3 fl_model/simulate_flower.py
170421	0.51	python3 fl_model/simulate_flower.py
547	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
174046	0.39	python3 fl_model/simulate_flower.py
174097	0.25	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
27959	0.21	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 5 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 50 round(s) in 4.23s
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=174932)[0m Memory growth enabled on GPUs

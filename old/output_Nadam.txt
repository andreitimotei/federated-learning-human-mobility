2025-04-12 13:26:13.318378: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-12 13:26:13.514986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744460773.592151  310551 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744460773.612877  310551 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744460773.780560  310551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460773.780630  310551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460773.780633  310551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460773.780635  310551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-12 13:26:13.800335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-12 13:26:20,656	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 3541632615.0, 'object_store_memory': 1770816307.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 6}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=311448)[0m 2025-04-12 13:26:21.722220: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=311448)[0m 2025-04-12 13:26:21.732451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=311448)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=311448)[0m E0000 00:00:1744460781.744579  311448 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=311448)[0m E0000 00:00:1744460781.748199  311448 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=311448)[0m W0000 00:00:1744460781.758071  311448 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=311448)[0m W0000 00:00:1744460781.758091  311448 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=311448)[0m W0000 00:00:1744460781.758092  311448 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=311448)[0m W0000 00:00:1744460781.758093  311448 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=311448)[0m 2025-04-12 13:26:21.761243: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=311448)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=311448)[0m I0000 00:00:1744460783.779546  311448 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=311448)[0m I0000 00:00:1744460787.483246  311659 service.cc:152] XLA service 0x7f3294002d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=311448)[0m I0000 00:00:1744460787.483288  311659 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(pid=311449)[0m 2025-04-12 13:26:21.722238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=311449)[0m 2025-04-12 13:26:21.734663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(ClientAppActor pid=311448)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=311449)[0m E0000 00:00:1744460781.746488  311449 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=311449)[0m E0000 00:00:1744460781.750334  311449 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=311449)[0m W0000 00:00:1744460781.759693  311449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster][0m
[36m(pid=311449)[0m 2025-04-12 13:26:21.762523: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=311449)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=311448)[0m 2025-04-12 13:26:27.566677: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=311448)[0m I0000 00:00:1744460787.894633  311659 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=311448)[0m 2025-04-12 13:26:28.769685: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=311448)[0m 
[36m(ClientAppActor pid=311449)[0m 
[36m(ClientAppActor pid=311449)[0m I0000 00:00:1744460784.613214  311449 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[36m(ClientAppActor pid=311448)[0m I0000 00:00:1744460791.444010  311659 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=311449)[0m 
[36m(ClientAppActor pid=311449)[0m I0000 00:00:1744460788.016866  311755 service.cc:152] XLA service 0x7fd8f8007300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=311449)[0m I0000 00:00:1744460788.016897  311755 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=311449)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(ClientAppActor pid=311449)[0m 2025-04-12 13:26:28.091970: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=311449)[0m I0000 00:00:1744460788.429227  311755 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=311448)[0m 
[36m(ClientAppActor pid=311448)[0m 2025-04-12 13:26:43.433509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=311449)[0m I0000 00:00:1744460791.755963  311755 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=311448)[0m 
[36m(ClientAppActor pid=311448)[0m 2025-04-12 13:27:24.973066: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=311449)[0m 2025-04-12 13:27:36.743315: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=311449)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=311449)[0m 2025-04-12 13:27:42.285334: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=311449)[0m 
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m [2025-04-12 13:28:20,609 E 310741 310741] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=311449)[0m 2025-04-12 13:28:21.823715: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=311449)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=311449)[0m 2025-04-12 13:29:20.030538: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=311449)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: c23dd014891070026f1fdeff01000000, name=ClientAppActor.__init__, pid=311448, memory used=1.86GB) was running was 22.26GB / 22.49GB (0.990018), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-ee5218ae93af9d0d7e36d025e53e18cef3c51196d218fb505d9b18b8*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
311449	1.90	ray::ClientAppActor.run
311448	1.86	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: f626da5b8713cd1f7e46f35d7178a77252a3cb1d9db55607e7d58cb5) where the task (actor ID: b22a4797a56748ef993875f701000000, name=ClientAppActor.__init__, pid=311449, memory used=3.78GB) was running was 22.28GB / 22.49GB (0.990615), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-bca390bb1d05c56898569f29e5baec28888f512494a849ef7681814d*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
311449	3.78	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.66	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
310551	0.48	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 188.53s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 316.3821165496324
[92mINFO [0m:      		round 2: 63.012596130371094
[92mINFO [0m:      		round 3: 46.42522048950195
[92mINFO [0m:      		round 4: 63.209781646728516
[92mINFO [0m:      		round 5: 84.02108001708984
[92mINFO [0m:      		round 6: 56.72206115722656
[92mINFO [0m:      		round 7: 98.86561584472656
[92mINFO [0m:      		round 8: 31.906139373779297
[92mINFO [0m:      		round 9: 40.949241638183594
[92mINFO [0m:      		round 10: 63.07534408569336
[92mINFO [0m:      		round 11: 62.807796478271484
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=311448)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=311448)[0m Full evaluation results: [288.6422424316406, 186.10816955566406, 706.3408203125, 0.0076068914495408535, 8.292017936706543, 26.548276901245117, 0.07795222103595734]
[36m(ClientAppActor pid=311448)[0m [CLIENT] Duration MAE = 8.2920, Lat MAE = 26.5483, Lon MAE = 0.0780
[36m(ClientAppActor pid=311449)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=311448)[0m Full evaluation results: [317.8625183105469, 246.2884063720703, 726.4193725585938, 0.003276913659647107, 8.918392181396484, 26.934106826782227, 0.04557144641876221][32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=311448)[0m [CLIENT] Duration MAE = 8.9184, Lat MAE = 26.9341, Lon MAE = 0.0456[32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [63.012596130371094, 151.5293426513672, 37.74776077270508, 0.007918217219412327, 9.268133163452148, 5.927163600921631, 0.07462955266237259][32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 9.2681, Lat MAE = 5.9272, Lon MAE = 0.0746[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [46.42522048950195, 117.44123840332031, 22.166091918945312, 0.00376012921333313, 8.012921333312988, 4.364124774932861, 0.053151313215494156]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 8.0129, Lat MAE = 4.3641, Lon MAE = 0.0532
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [63.209781646728516, 196.4166717529297, 1.2694082260131836, 0.002562945708632469, 8.807855606079102, 0.947758138179779, 0.038997940719127655]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 8.8079, Lat MAE = 0.9478, Lon MAE = 0.0390
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [84.02108001708984, 212.6189422607422, 10.914789199829102, 0.006379235535860062, 10.336115837097168, 2.831808090209961, 0.06862897425889969]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 10.3361, Lat MAE = 2.8318, Lon MAE = 0.0686
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [56.72206115722656, 168.16827392578125, 3.5776851177215576, 0.0010124894324690104, 10.308816909790039, 1.2740139961242676, 0.025439096614718437]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 10.3088, Lat MAE = 1.2740, Lon MAE = 0.0254
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [98.86561584472656, 276.15692138671875, 1.5080101490020752, 0.0020290021784603596, 9.663010597229004, 0.9540050029754639, 0.03404635563492775]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 9.6630, Lat MAE = 0.9540, Lon MAE = 0.0340
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [31.906139373779297, 84.41667938232422, 3.839587926864624, 0.001578917377628386, 7.024246692657471, 1.7180603742599487, 0.03133290633559227]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 7.0242, Lat MAE = 1.7181, Lon MAE = 0.0313
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [40.949241638183594, 134.25767517089844, 1.0062353610992432, 0.0027003646828234196, 7.8335418701171875, 0.776426374912262, 0.04226503521203995]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 7.8335, Lat MAE = 0.7764, Lon MAE = 0.0423
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [63.07534408569336, 176.15684509277344, 0.3209361433982849, 0.0013505984097719193, 9.206892013549805, 0.48225492238998413, 0.02987871691584587]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 9.2069, Lat MAE = 0.4823, Lon MAE = 0.0299
[36m(ClientAppActor pid=311449)[0m Full evaluation results: [62.807796478271484, 175.93695068359375, 0.5706610679626465, 0.002900976687669754, 9.159894943237305, 0.6143614649772644, 0.04783845320343971]
[36m(ClientAppActor pid=311449)[0m [CLIENT] Duration MAE = 9.1599, Lat MAE = 0.6144, Lon MAE = 0.0478

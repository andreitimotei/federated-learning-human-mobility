2025-04-12 13:13:42.205211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-12 13:13:42.399312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744460022.472409  280124 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744460022.492209  280124 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744460022.655633  280124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460022.655687  280124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460022.655690  280124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460022.655692  280124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-12 13:13:42.675245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-12 13:13:49,733	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 3591787316.0, 'object_store_memory': 1795893657.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 6}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=281010)[0m 2025-04-12 13:13:50.849191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=281010)[0m 2025-04-12 13:13:50.858860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=281010)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=281010)[0m E0000 00:00:1744460030.870702  281010 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=281010)[0m E0000 00:00:1744460030.874585  281010 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=281010)[0m W0000 00:00:1744460030.885940  281010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=281010)[0m W0000 00:00:1744460030.885966  281010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=281010)[0m W0000 00:00:1744460030.885967  281010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=281010)[0m W0000 00:00:1744460030.885969  281010 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=281010)[0m 2025-04-12 13:13:50.889359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=281010)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=281010)[0m I0000 00:00:1744460033.027355  281010 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=281010)[0m I0000 00:00:1744460036.603116  281227 service.cc:152] XLA service 0x7f67e8008090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=281010)[0m I0000 00:00:1744460036.603189  281227 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(pid=281011)[0m 2025-04-12 13:13:50.877428: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=281011)[0m 2025-04-12 13:13:50.888162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(ClientAppActor pid=281010)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=281011)[0m E0000 00:00:1744460030.901219  281011 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=281011)[0m E0000 00:00:1744460030.905333  281011 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=281011)[0m W0000 00:00:1744460030.914806  281011 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster][0m
[36m(pid=281011)[0m 2025-04-12 13:13:50.917753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=281011)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:13:56.675622: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=281010)[0m I0000 00:00:1744460037.058406  281227 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:13:57.854872: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 
[36m(ClientAppActor pid=281011)[0m 
[36m(ClientAppActor pid=281011)[0m I0000 00:00:1744460033.958068  281011 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[36m(ClientAppActor pid=281010)[0m I0000 00:00:1744460040.367480  281227 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=281010)[0m 
[36m(ClientAppActor pid=281011)[0m I0000 00:00:1744460037.161543  281334 service.cc:152] XLA service 0x7f1018007240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=281011)[0m I0000 00:00:1744460037.161598  281334 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=281011)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(ClientAppActor pid=281011)[0m 2025-04-12 13:13:57.222844: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=281011)[0m I0000 00:00:1744460037.602751  281334 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:14:19.203606: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=281011)[0m I0000 00:00:1744460040.695389  281334 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=281011)[0m 
[36m(ClientAppActor pid=281011)[0m 2025-04-12 13:14:27.417504: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:14:29.351967: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 
[36m(ClientAppActor pid=281011)[0m 
[36m(ClientAppActor pid=281011)[0m 2025-04-12 13:14:37.097697: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=281011)[0m 2025-04-12 13:14:51.958576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281011)[0m 
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m [2025-04-12 13:15:49,683 E 280303 280303] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:16:15.267630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:16:31.135126: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=281010)[0m 2025-04-12 13:16:41.683271: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=281010)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 048c518aab90c9c21bce4b8e01000000, name=ClientAppActor.__init__, pid=281010, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990075), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-b308c12ce0442a5e9845e1f4657f27d800b3eaeb3cda745493651aa7*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
281010	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
280124	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: b2dc9dc86923710213713005e59096f71b06d90530c8b0e7057033ea) where the task (actor ID: 50bbc63aa86c16578b9dbf8a01000000, name=ClientAppActor.__init__, pid=281011, memory used=1.93GB) was running was 22.32GB / 22.49GB (0.99265), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-fa4150549da05ae0e8501bfe48f71def0b28d3407df0e79e80289e29*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
281011	1.93	ray::ClientAppActor.run
281010	1.89	ray::ClientAppActor.run
112417	0.65	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.44	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 202.21s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 477.8682196792627
[92mINFO [0m:      		round 2: 68.6478500366211
[92mINFO [0m:      		round 3: 102.47392272949219
[92mINFO [0m:      		round 4: 77.60749816894531
[92mINFO [0m:      		round 5: 41.62199020385742
[92mINFO [0m:      		round 6: 57.16783142089844
[92mINFO [0m:      		round 7: 31.61387825012207
[92mINFO [0m:      		round 8: 51.05305862426758
[92mINFO [0m:      		round 9: 72.70054626464844
[92mINFO [0m:      		round 10: 63.1459846496582
[92mINFO [0m:      		round 11: 40.96902847290039
[92mINFO [0m:      		round 12: 54.83203125
[92mINFO [0m:      		round 13: 45.829200744628906
[92mINFO [0m:      		round 14: 71.27330017089844
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=281010)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [506.5467529296875, 268.4439392089844, 1229.4898681640625, 0.017465466633439064, 10.842353820800781, 35.09906005859375, 0.12568123638629913]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 10.8424, Lat MAE = 35.0991, Lon MAE = 0.1257
[36m(ClientAppActor pid=281011)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=281011)[0m Full evaluation results: [451.9844055175781, 179.20242309570312, 1208.16259765625, 0.009286940097808838, 8.087167739868164, 34.71469497680664, 0.08598589152097702][32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=281011)[0m [CLIENT] Duration MAE = 8.0872, Lat MAE = 34.7147, Lon MAE = 0.0860[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [68.6478500366211, 94.72735595703125, 103.78254699707031, 0.0019258596003055573, 7.024685859680176, 9.292914390563965, 0.035218775272369385][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 7.0247, Lat MAE = 9.2929, Lon MAE = 0.0352[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [102.47392272949219, 242.8370361328125, 34.93342590332031, 0.003556306241080165, 12.166781425476074, 5.020318508148193, 0.0488482303917408]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 12.1668, Lat MAE = 5.0203, Lon MAE = 0.0488
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [77.60749816894531, 205.7367706298828, 3.248330593109131, 0.0025146715342998505, 9.986650466918945, 1.471941351890564, 0.04003063216805458]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 9.9867, Lat MAE = 1.4719, Lon MAE = 0.0400
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [41.62199020385742, 133.8650665283203, 2.172973394393921, 0.0014059166423976421, 7.851322650909424, 1.3363628387451172, 0.029491182416677475]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 7.8513, Lat MAE = 1.3364, Lon MAE = 0.0295
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [57.16783142089844, 160.49935913085938, 0.8827494978904724, 0.000982552650384605, 9.479941368103027, 0.7944023609161377, 0.025721848011016846]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 9.4799, Lat MAE = 0.7944, Lon MAE = 0.0257
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [31.61387825012207, 86.98529815673828, 1.3919079303741455, 0.001008141553029418, 7.598649978637695, 0.9815073609352112, 0.0240129753947258]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 7.5986, Lat MAE = 0.9815, Lon MAE = 0.0240
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [51.05305862426758, 150.87557983398438, 1.314732551574707, 0.0017741314368322492, 8.607821464538574, 0.8955861926078796, 0.03348901867866516]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 8.6078, Lat MAE = 0.8956, Lon MAE = 0.0335
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [72.70054626464844, 218.4486083984375, 0.4535284638404846, 0.0022171963937580585, 10.782098770141602, 0.565101146697998, 0.03863418847322464]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 10.7821, Lat MAE = 0.5651, Lon MAE = 0.0386
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [63.1459846496582, 178.02110290527344, 0.4350154995918274, 0.0025700966361910105, 9.244073867797852, 0.49518918991088867, 0.04184773936867714]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 9.2441, Lat MAE = 0.4952, Lon MAE = 0.0418
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [40.96902847290039, 132.99166870117188, 1.1976855993270874, 0.004922798834741116, 8.074612617492676, 0.9111185669898987, 0.06471806019544601]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 8.0746, Lat MAE = 0.9111, Lon MAE = 0.0647
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [54.83203125, 149.23358154296875, 0.8911134004592896, 0.0011492704506963491, 9.233447074890137, 0.749659538269043, 0.029314860701560974]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 9.2334, Lat MAE = 0.7497, Lon MAE = 0.0293
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [45.829200744628906, 135.93043518066406, 0.1757453978061676, 0.0014826097758486867, 8.682075500488281, 0.3462598919868469, 0.029800457879900932]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 8.6821, Lat MAE = 0.3463, Lon MAE = 0.0298
[36m(ClientAppActor pid=281010)[0m Full evaluation results: [71.27330017089844, 191.7075653076172, 0.658767819404602, 0.0039060532581061125, 9.81257152557373, 0.6731449961662292, 0.05623212456703186]
[36m(ClientAppActor pid=281010)[0m [CLIENT] Duration MAE = 9.8126, Lat MAE = 0.6731, Lon MAE = 0.0562

2025-04-05 15:49:13.803691: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-05 15:49:13.973463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743864554.041326   28821 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743864554.059240   28821 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1743864554.206636   28821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743864554.206700   28821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743864554.206703   28821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1743864554.206704   28821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-05 15:49:14.222722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=10, no round_timeout
2025-04-05 15:49:20,033	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 8.0, 'object_store_memory': 3167955763.0, 'memory': 6335911527.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 8 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=29584)[0m 2025-04-05 15:49:21.178948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=29584)[0m 2025-04-05 15:49:21.188194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=29582)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=29582)[0m E0000 00:00:1743864561.199539   29582 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=29583)[0m E0000 00:00:1743864561.234047   29583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=29583)[0m W0000 00:00:1743864561.250615   29583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=29583)[0m W0000 00:00:1743864561.250645   29583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=29583)[0m W0000 00:00:1743864561.250648   29583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=29583)[0m W0000 00:00:1743864561.250651   29583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=29583)[0m 2025-04-05 15:49:21.256883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=29583)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.
[36m(pid=29579)[0m 2025-04-05 15:49:21.204429: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=29579)[0m 2025-04-05 15:49:21.214571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered[32m [repeated 7x across cluster][0m
[36m(pid=29581)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 7x across cluster][0m
[36m(pid=29581)[0m E0000 00:00:1743864561.214386   29581 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered[32m [repeated 7x across cluster][0m
[36m(pid=29581)[0m E0000 00:00:1743864561.223243   29581 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered[32m [repeated 7x across cluster][0m
[36m(pid=29581)[0m W0000 00:00:1743864561.246290   29581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 28x across cluster][0m
[36m(pid=29581)[0m 2025-04-05 15:49:21.252115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 7x across cluster][0m
[36m(pid=29581)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 7x across cluster][0m
[36m(ClientAppActor pid=29584)[0m 2025-04-05 15:49:26.983166: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 17x across cluster][0m
[36m(ClientAppActor pid=29581)[0m 2025-04-05 15:49:27.282252: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected[32m [repeated 7x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 22x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29581)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 38x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 14x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29581)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 42x across cluster][0m
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 12x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 32x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29582)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 20x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 14x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29577)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 38x across cluster][0m
[36m(ClientAppActor pid=29577)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 16x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 22x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29578)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 30x across cluster][0m
[92mINFO [0m:      aggregate_fit: received 30 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 14x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 30 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 7 results and 23 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 38x across cluster][0m
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 7 results and 23 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 7 results and 23 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 7 results and 23 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[36m(ClientAppActor pid=29584)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 21x across cluster][0m
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 7 results and 23 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 7 results and 23 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=29583)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 14x across cluster][0m
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 7 results and 23 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 30 clients (out of 30)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587) where the task (actor ID: 781a76160c9df101946c80fe01000000, name=ClientAppActor.__init__, pid=29580, memory used=0.90GB) was running was 10.93GB / 11.40GB (0.958566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-4136de3fdde6d3deda3935e0a181449a655b4d219ecbc19f69976872*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
29584	1.05	ray::ClientAppActor.run
29581	1.05	ray::ClientAppActor.run
29583	1.01	ray::ClientAppActor.run
29577	1.01	ray::ClientAppActor.run
29582	0.99	ray::ClientAppActor.run
29579	0.98	ray::ClientAppActor.run
29578	0.94	ray::ClientAppActor.run
29580	0.90	ray::ClientAppActor.run
811	0.44	/home/timotei/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/timotei/.vscode...
28821	0.37	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 7 results and 23 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 10 round(s) in 102.08s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 592.4436645507812
[92mINFO [0m:      		round 2: 589.4090576171875
[92mINFO [0m:      		round 3: 558.1159057617188
[92mINFO [0m:      		round 4: 529.7208862304688
[92mINFO [0m:      		round 5: 523.7542114257812
[92mINFO [0m:      		round 6: 524.1080322265625
[92mINFO [0m:      		round 7: 501.71954345703125
[92mINFO [0m:      		round 8: 479.42852783203125
[92mINFO [0m:      		round 9: 455.6558532714844
[92mINFO [0m:      		round 10: 437.34844970703125
[92mINFO [0m:      
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=592.4437, MAE=575.0330, Acc=6.9202
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=589.4091, MAE=571.5466, Acc=6.8104[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29580)[0m [CLIENT] Eval loss=558.1159, MAE=540.3451, Acc=6.5015[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29581)[0m [CLIENT] Eval loss=529.7209, MAE=512.1940, Acc=6.2154[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=523.7542, MAE=506.5297, Acc=6.0786[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29577)[0m [CLIENT] Eval loss=524.1080, MAE=507.0733, Acc=6.0580[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=501.7195, MAE=485.2748, Acc=5.8965[32m [repeated 30x across cluster][0m
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=455.6559, MAE=440.1802, Acc=5.6356[32m [repeated 14x across cluster][0m
[36m(ClientAppActor pid=29583)[0m [CLIENT] Eval loss=437.3484, MAE=421.6548, Acc=5.5697[32m [repeated 7x across cluster][0m
[33m(raylet)[0m [2025-04-05 15:51:19,977 E 29017 29017] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 77b66bba1b8c4888ee3b78209440a95004a7e2200f4ae86054796587, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=29581)[0m [93mWARNING [0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.[32m [repeated 13x across cluster][0m
[36m(ClientAppActor pid=29577)[0m [CLIENT] Eval loss=437.3484, MAE=421.6548, Acc=5.5697[32m [repeated 6x across cluster][0m

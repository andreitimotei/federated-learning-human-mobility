2025-04-07 17:29:24.673124: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 17:29:24.861559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744043364.933618   84313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744043364.953764   84313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744043365.111348   84313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744043365.111415   84313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744043365.111418   84313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744043365.111420   84313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 17:29:25.130842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=10, no round_timeout
2025-04-07 17:29:31,669	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 8.0, 'memory': 5944423220.0, 'object_store_memory': 2972211609.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 4}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=85066)[0m 2025-04-07 17:29:32.739862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=85066)[0m 2025-04-07 17:29:32.752555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=85066)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=85066)[0m E0000 00:00:1744043372.767420   85066 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=85066)[0m E0000 00:00:1744043372.771800   85066 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=85066)[0m W0000 00:00:1744043372.783941   85066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=85066)[0m W0000 00:00:1744043372.783973   85066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=85066)[0m W0000 00:00:1744043372.783975   85066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=85066)[0m W0000 00:00:1744043372.783976   85066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=85066)[0m 2025-04-07 17:29:32.787376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=85066)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=85067)[0m I0000 00:00:1744043374.714631   85067 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[36m(ClientAppActor pid=85067)[0m I0000 00:00:1744043376.948647   85282 service.cc:152] XLA service 0x7fdf88016820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=85067)[0m I0000 00:00:1744043376.948694   85282 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=85067)[0m 2025-04-07 17:29:36.999492: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=85067)[0m I0000 00:00:1744043377.296127   85282 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(pid=85067)[0m 2025-04-07 17:29:32.739833: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=85067)[0m 2025-04-07 17:29:32.752570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(ClientAppActor pid=85066)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=85067)[0m E0000 00:00:1744043372.767469   85067 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=85067)[0m E0000 00:00:1744043372.771784   85067 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=85067)[0m W0000 00:00:1744043372.783976   85067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster][0m
[36m(pid=85067)[0m 2025-04-07 17:29:32.787376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=85067)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=85067)[0m 2025-04-07 17:29:38.318773: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 52 bytes spill stores, 52 bytes spill loads
[36m(ClientAppActor pid=85067)[0m 
[36m(ClientAppActor pid=85067)[0m 2025-04-07 17:29:38.413312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 440 bytes spill stores, 440 bytes spill loads
[36m(ClientAppActor pid=85067)[0m 
[36m(ClientAppActor pid=85066)[0m 
[36m(ClientAppActor pid=85066)[0m 
[36m(ClientAppActor pid=85067)[0m I0000 00:00:1744043379.961893   85282 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=85066)[0m I0000 00:00:1744043375.531241   85066 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[36m(ClientAppActor pid=85067)[0m 
[36m(ClientAppActor pid=85067)[0m 
[36m(ClientAppActor pid=85066)[0m 
[36m(ClientAppActor pid=85066)[0m 
[36m(ClientAppActor pid=85066)[0m I0000 00:00:1744043377.536011   85406 service.cc:152] XLA service 0x7fd6ac005980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=85066)[0m I0000 00:00:1744043377.536046   85406 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[92mINFO [0m:      aggregate_fit: received 20 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_evaluate: received 20 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_fit: received 20 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_evaluate: received 20 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_fit: received 20 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_evaluate: received 20 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_fit: received 20 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[92mINFO [0m:      aggregate_evaluate: received 20 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 9 results and 11 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 19 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 19 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 19 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 19 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 19 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 20 clients (out of 20)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: d392b96d76ee94a67ebff58dfbc0c440cc3601517afa7ed2af6fd2a7) where the task (actor ID: d527340f2d7d1f8681fd64ca01000000, name=ClientAppActor.__init__, pid=85067, memory used=4.67GB) was running was 11.29GB / 11.40GB (0.99041), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-985207c5832f448ce7916621e7696713642adaed63e8af063975bc3f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
85066	4.69	ray::ClientAppActor.run
85067	4.67	ray::ClientAppActor.run
632	0.25	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
84376	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
84511	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
827	0.13	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
84459	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
84313	0.06	python3 fl_model/simulate_flower.py
84458	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/autoscaler/_pr...
84512	0.05	/home/timotei/venv/bin/python3 -u /home/timotei/venv/lib/python3.12/site-packages/ray/_private/log_m...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 19 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 10 round(s) in 323.16s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 287.39007568359375
[92mINFO [0m:      		round 2: 303.16497802734375
[92mINFO [0m:      		round 3: 312.2431945800781
[92mINFO [0m:      		round 4: 313.4037780761719
[92mINFO [0m:      		round 5: 311.24530029296875
[92mINFO [0m:      		round 6: 293.4438171386719
[92mINFO [0m:      		round 7: 264.2964782714844
[92mINFO [0m:      		round 8: 263.9918518066406
[92mINFO [0m:      		round 9: 268.5897521972656
[92mINFO [0m:      		round 10: 241.47386169433594
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=85067)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [287.39007568359375, 464.64892578125, 6.588191986083984, 0.004651162773370743, 12.82868480682373]
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0047, Destination Accuracy = 12.8287
[36m(ClientAppActor pid=85066)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [287.39007568359375, 464.64892578125, 6.588191986083984, 0.004651162773370743, 12.82868480682373][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0047, Destination Accuracy = 12.8287[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [303.16497802734375, 490.8792419433594, 6.446865081787109, 0.041860464960336685, 13.810916900634766][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0419, Destination Accuracy = 13.8109[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [303.16497802734375, 490.8792419433594, 6.446865081787109, 0.041860464960336685, 13.810916900634766][32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0419, Destination Accuracy = 13.8109[32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [303.16497802734375, 490.8792419433594, 6.446865081787109, 0.041860464960336685, 13.810916900634766][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0419, Destination Accuracy = 13.8109[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [312.2431945800781, 505.79022216796875, 6.365259647369385, 0.03953488543629646, 14.347089767456055][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0395, Destination Accuracy = 14.3471[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [312.2431945800781, 505.79022216796875, 6.365259647369385, 0.03953488543629646, 14.347089767456055][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0395, Destination Accuracy = 14.3471[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [313.4037780761719, 507.81243896484375, 6.295098304748535, 0.011627906933426857, 14.403390884399414][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0116, Destination Accuracy = 14.4034[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m Full evaluation results: [313.4037780761719, 507.81243896484375, 6.295098304748535, 0.011627906933426857, 14.403390884399414][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85067)[0m [CLIENT] Duration MAE = 0.0116, Destination Accuracy = 14.4034[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [311.24530029296875, 504.4623718261719, 6.221954345703125, 0.025581395253539085, 14.286798477172852][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0256, Destination Accuracy = 14.2868[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [293.4438171386719, 475.6479187011719, 5.865201473236084, 0.044186048209667206, 13.267534255981445]
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0442, Destination Accuracy = 13.2675
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [264.2964782714844, 427.5247497558594, 5.629385471343994, 0.0325581394135952, 11.394354820251465]
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 11.3944
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [263.9918518066406, 426.761474609375, 5.567324638366699, 0.03953488543629646, 11.360505104064941]
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0395, Destination Accuracy = 11.3605
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [268.5897521972656, 434.1531677246094, 5.637267112731934, 0.041860464960336685, 11.699857711791992]
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0419, Destination Accuracy = 11.6999
[36m(ClientAppActor pid=85066)[0m 2025-04-07 17:29:37.577135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=85066)[0m I0000 00:00:1744043377.878903   85406 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=85066)[0m 2025-04-07 17:29:41.937540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1497', 268 bytes spill stores, 268 bytes spill loads[32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=85066)[0m I0000 00:00:1744043380.470986   85406 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=85066)[0m Full evaluation results: [241.47386169433594, 389.650146484375, 5.643736839294434, 0.04651162773370743, 10.15615463256836]
[36m(ClientAppActor pid=85066)[0m [CLIENT] Duration MAE = 0.0465, Destination Accuracy = 10.1562

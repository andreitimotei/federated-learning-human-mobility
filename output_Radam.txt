2025-04-12 13:22:49.834278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-12 13:22:49.965234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744460570.026561  308806 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744460570.047968  308806 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744460570.182271  308806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460570.182341  308806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460570.182344  308806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460570.182346  308806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-12 13:22:50.202787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-12 13:22:57,241	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 3569103668.0, 'object_store_memory': 1784551833.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 6}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=309697)[0m 2025-04-12 13:22:58.396629: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=309697)[0m 2025-04-12 13:22:58.407056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=309697)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=309697)[0m E0000 00:00:1744460578.419698  309697 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=309697)[0m E0000 00:00:1744460578.423450  309697 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=309697)[0m W0000 00:00:1744460578.435475  309697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=309697)[0m W0000 00:00:1744460578.435517  309697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=309697)[0m W0000 00:00:1744460578.435519  309697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=309697)[0m W0000 00:00:1744460578.435521  309697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=309697)[0m 2025-04-12 13:22:58.438852: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=309697)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=309697)[0m I0000 00:00:1744460580.489299  309697 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 144, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 128, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py", line 119, in handle_legacy_message_from_msgtype
    get_parameters_res = maybe_call_get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client.py", line 204, in maybe_call_get_parameters
    return client.get_parameters(get_parameters_ins)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/numpy_client.py", line 214, in _get_parameters
    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 28, in get_parameters
    client = self._load_client()
             ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 42, in _load_client
    return FederatedClient(path_to_data=path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/client.py", line 48, in __init__
    self.model = create_model_complex(input_shape=(self.X_train.shape[1],))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/model.py", line 144, in create_model_complex
    optimizer=tf.keras.optimizers.RAdam(learning_rate=0.001),
              ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'keras.api.optimizers' has no attribute 'RAdam'. Did you mean: 'Adam'?

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: module 'keras.api.optimizers' has no attribute 'RAdam'

[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 144, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 128, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py", line 119, in handle_legacy_message_from_msgtype
    get_parameters_res = maybe_call_get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client.py", line 204, in maybe_call_get_parameters
    return client.get_parameters(get_parameters_ins)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/numpy_client.py", line 214, in _get_parameters
    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 28, in get_parameters
    client = self._load_client()
             ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 42, in _load_client
    return FederatedClient(path_to_data=path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/client.py", line 48, in __init__
    self.model = create_model_complex(input_shape=(self.X_train.shape[1],))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/model.py", line 144, in create_model_complex
    optimizer=tf.keras.optimizers.RAdam(learning_rate=0.001),
              ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'keras.api.optimizers' has no attribute 'RAdam'. Did you mean: 'Adam'?

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: module 'keras.api.optimizers' has no attribute 'RAdam'
[91mERROR [0m:     [36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 144, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 128, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py", line 119, in handle_legacy_message_from_msgtype
    get_parameters_res = maybe_call_get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client.py", line 204, in maybe_call_get_parameters
    return client.get_parameters(get_parameters_ins)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/numpy_client.py", line 214, in _get_parameters
    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 28, in get_parameters
    client = self._load_client()
             ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 42, in _load_client
    return FederatedClient(path_to_data=path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/client.py", line 48, in __init__
    self.model = create_model_complex(input_shape=(self.X_train.shape[1],))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/model.py", line 144, in create_model_complex
    optimizer=tf.keras.optimizers.RAdam(learning_rate=0.001),
              ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'keras.api.optimizers' has no attribute 'RAdam'. Did you mean: 'Adam'?

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: module 'keras.api.optimizers' has no attribute 'RAdam'
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/legacy_app.py", line 361, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 93, in fit
    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 284, in _get_initial_parameters
    get_parameters_res = random_client.get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 170, in get_parameters
    message_out = self._submit_job(message, timeout)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 109, in _submit_job
    raise ex
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 144, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 128, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py", line 119, in handle_legacy_message_from_msgtype
    get_parameters_res = maybe_call_get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client.py", line 204, in maybe_call_get_parameters
    return client.get_parameters(get_parameters_ins)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/numpy_client.py", line 214, in _get_parameters
    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 28, in get_parameters
    client = self._load_client()
             ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 42, in _load_client
    return FederatedClient(path_to_data=path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/client.py", line 48, in __init__
    self.model = create_model_complex(input_shape=(self.X_train.shape[1],))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/model.py", line 144, in create_model_complex
    optimizer=tf.keras.optimizers.RAdam(learning_rate=0.001),
              ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'keras.api.optimizers' has no attribute 'RAdam'. Did you mean: 'Adam'?

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: module 'keras.api.optimizers' has no attribute 'RAdam'

[91mERROR [0m:     Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 6} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 6}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
Memory growth enabled on GPUs
[36m(ClientAppActor pid=309697)[0m Memory growth enabled on GPUs
Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/legacy_app.py", line 361, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 93, in fit
    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/server/server.py", line 284, in _get_initial_parameters
    get_parameters_res = random_client.get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 170, in get_parameters
    message_out = self._submit_job(message, timeout)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 109, in _submit_job
    raise ex
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 144, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client_app.py", line 128, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py", line 119, in handle_legacy_message_from_msgtype
    get_parameters_res = maybe_call_get_parameters(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/client.py", line 204, in maybe_call_get_parameters
    return client.get_parameters(get_parameters_ins)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/client/numpy_client.py", line 214, in _get_parameters
    parameters = self.numpy_client.get_parameters(config=ins.config)  # type: ignore
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 28, in get_parameters
    client = self._load_client()
             ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 42, in _load_client
    return FederatedClient(path_to_data=path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/client.py", line 48, in __init__
    self.model = create_model_complex(input_shape=(self.X_train.shape[1],))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/federated-learning-human-mobility/fl_model/model.py", line 144, in create_model_complex
    optimizer=tf.keras.optimizers.RAdam(learning_rate=0.001),
              ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'keras.api.optimizers' has no attribute 'RAdam'. Did you mean: 'Adam'?

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=309697, ip=192.168.52.78, actor_id=e9df9737f713def787c454cb01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f8ad17028a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: module 'keras.api.optimizers' has no attribute 'RAdam'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/timotei/federated-learning-human-mobility/fl_model/simulate_flower.py", line 53, in <module>
    fl.simulation.start_simulation(
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/legacy_app.py", line 397, in start_simulation
    raise RuntimeError("Simulation crashed.") from ex
RuntimeError: Simulation crashed.
[36m(pid=309698)[0m 2025-04-12 13:22:58.396634: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=309698)[0m 2025-04-12 13:22:58.407722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=309698)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=309698)[0m E0000 00:00:1744460578.422237  309698 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=309698)[0m E0000 00:00:1744460578.425774  309698 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=309698)[0m W0000 00:00:1744460578.437361  309698 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=309698)[0m 2025-04-12 13:22:58.440876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=309698)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
